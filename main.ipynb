{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.datasets import FashionMNIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out, stride):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(ch_in, ch_out,\n",
    "                              kernel_size=(3, 3), stride=stride)\n",
    "        self.bn = nn.BatchNorm2d(ch_out)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.conv(input)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "\n",
    "        layer_config = ((64, 2), (64, 1), (128, 2), (128, 1))\n",
    "\n",
    "        ch_in = 1\n",
    "        block_list = []\n",
    "        for ch_out, stride in layer_config:\n",
    "            block = ConvBlock(ch_in, ch_out, stride)\n",
    "            block_list.append(block)\n",
    "            ch_in = ch_out\n",
    "\n",
    "        self.backbone = nn.Sequential(*block_list)\n",
    "\n",
    "        self.head = nn.Linear(layer_config[-1][0], num_classes)\n",
    "\n",
    "    def forward(self, input):\n",
    "        featuremap = self.backbone(input)\n",
    "        squashed = F.adaptive_avg_pool2d(featuremap, output_size=(1, 1))\n",
    "        squeezed = squashed.view(squashed.shape[0], -1)\n",
    "        pred = self.head(squeezed)\n",
    "        return pred\n",
    "\n",
    "    @classmethod\n",
    "    def loss(cls, pred, gt):\n",
    "        return F.cross_entropy(pred, gt)\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.train_transform = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomResizedCrop(size=(28, 28), scale=(0.7, 1.1)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        self.val_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "        train_dataset = FashionMNIST(\"./data\", train=True,\n",
    "                                     transform=self.train_transform,\n",
    "                                     download=True)\n",
    "        val_dataset = FashionMNIST(\"./data\", train=False,\n",
    "                                   transform=self.val_transform,\n",
    "                                   download=True)\n",
    "\n",
    "        batch_size = 1024\n",
    "        self.train_loader = data.DataLoader(train_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True, num_workers=4)\n",
    "        self.val_loader = data.DataLoader(val_dataset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=4)\n",
    "\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        self.net = NeuralNet()\n",
    "        self.net.to(self.device)\n",
    "\n",
    "        self.logger = SummaryWriter()\n",
    "        self.i_batch = 0\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        num_epochs = 100\n",
    "\n",
    "        optimizer = torch.optim.Adam(self.net.parameters(), lr=1e-3)\n",
    "\n",
    "        for i_epoch in range(num_epochs):\n",
    "            self.net.train()\n",
    "\n",
    "            for feature_batch, gt_batch in self.train_loader:\n",
    "                feature_batch = feature_batch.to(self.device)\n",
    "                gt_batch = gt_batch.to(self.device)\n",
    "\n",
    "                pred_batch = self.net(feature_batch)\n",
    "\n",
    "                loss = NeuralNet.loss(pred_batch, gt_batch)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                self.logger.add_scalar(\"train/loss\", loss.item(), self.i_batch)\n",
    "\n",
    "                if self.i_batch % 100 == 0:\n",
    "                    print(f\"batch={self.i_batch} loss={loss.item():.6f}\")\n",
    "\n",
    "                self.i_batch += 1\n",
    "\n",
    "            self.validate()\n",
    "\n",
    "            torch.save(self.net, \"mymodel.pth\")\n",
    "\n",
    "    def validate(self):\n",
    "        self.net.eval()\n",
    "\n",
    "        loss_all = []\n",
    "        pred_all = []\n",
    "        gt_all = []\n",
    "        for feature_batch, gt_batch in self.val_loader:\n",
    "            feature_batch = feature_batch.to(self.device)\n",
    "            gt_batch = gt_batch.to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                pred_batch = self.net(feature_batch)\n",
    "                loss = NeuralNet.loss(pred_batch, gt_batch)\n",
    "\n",
    "            loss_all.append(loss.item())\n",
    "            pred_all.append(pred_batch.cpu().numpy())\n",
    "            gt_all.append(gt_batch.cpu().numpy())\n",
    "\n",
    "        loss_mean = np.mean(np.array(loss_all))\n",
    "        pred_all = np.argmax(np.concatenate(pred_all, axis=0), axis=1)\n",
    "        gt_all = np.concatenate(np.array(gt_all))\n",
    "\n",
    "        accuracy = np.sum(np.equal(pred_all, gt_all)) / len(pred_all)\n",
    "\n",
    "        self.logger.add_scalar(\"val/loss\", loss_mean, self.i_batch)\n",
    "        self.logger.add_scalar(\"val/accuracy\", accuracy, self.i_batch)\n",
    "\n",
    "        print(f\"Val_loss={loss_mean} val_accu={accuracy:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0.7%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "batch=0 loss=2.368594\n",
      "Val_loss=0.6340932726860047 val_accu=0.782000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_86326/3119641597.py:130: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  gt_all = np.concatenate(np.array(gt_all))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch=100 loss=0.529306\n",
      "Val_loss=0.46321995854377745 val_accu=0.839000\n",
      "Val_loss=0.41024490296840666 val_accu=0.851900\n",
      "batch=200 loss=0.409693\n",
      "Val_loss=0.40409609079360964 val_accu=0.854300\n",
      "Val_loss=0.39032872319221495 val_accu=0.859500\n",
      "batch=300 loss=0.400739\n",
      "Val_loss=0.34073994755744935 val_accu=0.880300\n",
      "batch=400 loss=0.317761\n",
      "Val_loss=0.34931646287441254 val_accu=0.874200\n",
      "Val_loss=0.3235402464866638 val_accu=0.883600\n",
      "batch=500 loss=0.296845\n",
      "Val_loss=0.31485940515995026 val_accu=0.889200\n",
      "Val_loss=0.29493704438209534 val_accu=0.896400\n",
      "batch=600 loss=0.304333\n",
      "Val_loss=0.32510591447353365 val_accu=0.879900\n",
      "batch=700 loss=0.278693\n",
      "Val_loss=0.28910290002822875 val_accu=0.896400\n",
      "Val_loss=0.27643261551856996 val_accu=0.901700\n",
      "batch=800 loss=0.306322\n",
      "Val_loss=0.280517090857029 val_accu=0.897800\n",
      "Val_loss=0.2794510066509247 val_accu=0.900000\n",
      "batch=900 loss=0.258641\n",
      "Val_loss=0.26627833396196365 val_accu=0.904900\n",
      "batch=1000 loss=0.289317\n",
      "Val_loss=0.27063893973827363 val_accu=0.902300\n",
      "Val_loss=0.2641278669238091 val_accu=0.907600\n",
      "batch=1100 loss=0.282069\n",
      "Val_loss=0.2560072034597397 val_accu=0.908300\n",
      "Val_loss=0.2585488632321358 val_accu=0.905300\n",
      "batch=1200 loss=0.229488\n",
      "Val_loss=0.29341574013233185 val_accu=0.890700\n",
      "Val_loss=0.2581730142235756 val_accu=0.906400\n",
      "batch=1300 loss=0.247267\n",
      "Val_loss=0.24836426675319673 val_accu=0.911000\n",
      "batch=1400 loss=0.273097\n",
      "Val_loss=0.24454803317785262 val_accu=0.913800\n",
      "Val_loss=0.24856558442115784 val_accu=0.912800\n",
      "batch=1500 loss=0.214689\n",
      "Val_loss=0.25966463685035707 val_accu=0.905900\n",
      "Val_loss=0.26343866884708406 val_accu=0.902300\n",
      "batch=1600 loss=0.232867\n",
      "Val_loss=0.24933236986398696 val_accu=0.912300\n",
      "batch=1700 loss=0.216365\n",
      "Val_loss=0.27762474715709684 val_accu=0.900200\n",
      "Val_loss=0.23365456759929656 val_accu=0.913900\n",
      "batch=1800 loss=0.263154\n",
      "Val_loss=0.24312580972909928 val_accu=0.915400\n",
      "Val_loss=0.2385663643479347 val_accu=0.918100\n",
      "batch=1900 loss=0.220076\n",
      "Val_loss=0.23936969488859178 val_accu=0.915000\n",
      "batch=2000 loss=0.200796\n",
      "Val_loss=0.2549763396382332 val_accu=0.909900\n",
      "Val_loss=0.24354414343833924 val_accu=0.912500\n",
      "batch=2100 loss=0.241330\n",
      "Val_loss=0.23555154204368592 val_accu=0.917700\n",
      "Val_loss=0.2481023833155632 val_accu=0.909500\n",
      "batch=2200 loss=0.191836\n",
      "Val_loss=0.27940885722637177 val_accu=0.900800\n",
      "batch=2300 loss=0.199647\n",
      "Val_loss=0.23150927275419236 val_accu=0.915700\n",
      "Val_loss=0.22760113030672074 val_accu=0.917500\n",
      "batch=2400 loss=0.194981\n",
      "Val_loss=0.23213210254907607 val_accu=0.919800\n",
      "Val_loss=0.2367338314652443 val_accu=0.915500\n",
      "batch=2500 loss=0.231099\n",
      "Val_loss=0.24393269866704942 val_accu=0.912300\n",
      "Val_loss=0.22146959900856017 val_accu=0.920300\n",
      "batch=2600 loss=0.205811\n",
      "Val_loss=0.24786050319671632 val_accu=0.912800\n",
      "batch=2700 loss=0.198654\n",
      "Val_loss=0.22552878856658937 val_accu=0.917300\n",
      "Val_loss=0.2256681129336357 val_accu=0.917900\n",
      "batch=2800 loss=0.183191\n",
      "Val_loss=0.24615287631750107 val_accu=0.913900\n",
      "Val_loss=0.2330360695719719 val_accu=0.918800\n",
      "batch=2900 loss=0.219161\n",
      "Val_loss=0.24166931509971618 val_accu=0.917000\n",
      "batch=3000 loss=0.196443\n",
      "Val_loss=0.23165703117847442 val_accu=0.917900\n",
      "Val_loss=0.2178436517715454 val_accu=0.922600\n",
      "batch=3100 loss=0.185346\n",
      "Val_loss=0.2253156930208206 val_accu=0.917900\n",
      "Val_loss=0.23238161355257034 val_accu=0.917200\n",
      "batch=3200 loss=0.197228\n",
      "Val_loss=0.2240847036242485 val_accu=0.919000\n",
      "batch=3300 loss=0.182406\n",
      "Val_loss=0.22991101443767548 val_accu=0.918600\n",
      "Val_loss=0.22339650988578796 val_accu=0.920600\n",
      "batch=3400 loss=0.208464\n",
      "Val_loss=0.22717396765947342 val_accu=0.918100\n",
      "Val_loss=0.2248782753944397 val_accu=0.919100\n",
      "batch=3500 loss=0.212266\n",
      "Val_loss=0.24111819863319398 val_accu=0.914100\n",
      "Val_loss=0.22426712661981582 val_accu=0.922200\n",
      "batch=3600 loss=0.176529\n",
      "Val_loss=0.22039292603731156 val_accu=0.920600\n",
      "batch=3700 loss=0.192747\n",
      "Val_loss=0.21756521761417388 val_accu=0.924200\n",
      "Val_loss=0.2139861151576042 val_accu=0.925200\n",
      "batch=3800 loss=0.156682\n",
      "Val_loss=0.2264355644583702 val_accu=0.918100\n",
      "Val_loss=0.2196282148361206 val_accu=0.924300\n",
      "batch=3900 loss=0.199022\n",
      "Val_loss=0.21652330905199052 val_accu=0.923600\n",
      "batch=4000 loss=0.177892\n",
      "Val_loss=0.2295352876186371 val_accu=0.917400\n",
      "Val_loss=0.21397629529237747 val_accu=0.926700\n",
      "batch=4100 loss=0.192490\n",
      "Val_loss=0.21817571073770523 val_accu=0.921700\n",
      "Val_loss=0.22260744273662567 val_accu=0.920400\n",
      "batch=4200 loss=0.155914\n",
      "Val_loss=0.21498670428991318 val_accu=0.925700\n",
      "batch=4300 loss=0.164951\n",
      "Val_loss=0.21824990063905716 val_accu=0.923600\n",
      "Val_loss=0.22242362946271896 val_accu=0.920700\n",
      "batch=4400 loss=0.180338\n",
      "Val_loss=0.21472658962011337 val_accu=0.924800\n",
      "Val_loss=0.24390729516744614 val_accu=0.911600\n",
      "batch=4500 loss=0.157356\n",
      "Val_loss=0.23499056845903396 val_accu=0.917300\n",
      "batch=4600 loss=0.166842\n",
      "Val_loss=0.2299409031867981 val_accu=0.917400\n",
      "Val_loss=0.23806810677051543 val_accu=0.914500\n",
      "batch=4700 loss=0.165795\n",
      "Val_loss=0.21123589277267457 val_accu=0.925400\n",
      "Val_loss=0.22686458230018616 val_accu=0.918300\n",
      "batch=4800 loss=0.157407\n",
      "Val_loss=0.21309590786695481 val_accu=0.926800\n",
      "Val_loss=0.22482465952634811 val_accu=0.920200\n",
      "batch=4900 loss=0.155010\n",
      "Val_loss=0.22501742392778395 val_accu=0.919100\n",
      "batch=5000 loss=0.147625\n",
      "Val_loss=0.2279038682579994 val_accu=0.921100\n",
      "Val_loss=0.23564544171094895 val_accu=0.916400\n",
      "batch=5100 loss=0.162877\n",
      "Val_loss=0.22147558480501175 val_accu=0.921500\n",
      "Val_loss=0.2027804136276245 val_accu=0.928800\n",
      "batch=5200 loss=0.155708\n",
      "Val_loss=0.2224149748682976 val_accu=0.923800\n",
      "batch=5300 loss=0.167269\n",
      "Val_loss=0.21867017447948456 val_accu=0.923700\n",
      "Val_loss=0.21733358800411223 val_accu=0.924500\n",
      "batch=5400 loss=0.163516\n",
      "Val_loss=0.2357308641076088 val_accu=0.919500\n",
      "Val_loss=0.22215152382850648 val_accu=0.922400\n",
      "batch=5500 loss=0.163887\n",
      "Val_loss=0.21488402038812637 val_accu=0.925500\n",
      "batch=5600 loss=0.162909\n",
      "Val_loss=0.2164565160870552 val_accu=0.923700\n",
      "Val_loss=0.21591685861349105 val_accu=0.924900\n",
      "batch=5700 loss=0.166722\n",
      "Val_loss=0.21257335543632508 val_accu=0.925500\n",
      "Val_loss=0.20919608175754548 val_accu=0.927900\n",
      "batch=5800 loss=0.151530\n",
      "Val_loss=0.22198835909366607 val_accu=0.925700\n",
      "Val_loss=0.21145805716514587 val_accu=0.925600\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer = Trainer()\n",
    "trainer.train()\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
