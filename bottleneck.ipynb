{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.datasets import FashionMNIST\n",
    "\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAMES = [\"T-shirt or top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "COLORS = [\"red\", \"green\", \"blue\", \"yellow\", \"aqua\", \"navy\", \"maroon\", \"magenta\", \"orange\", \"crimson\"]\n",
    "ENUM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out, stride):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(ch_in, ch_out,\n",
    "                              kernel_size=(3, 3), stride=stride)\n",
    "        self.bn = nn.BatchNorm2d(ch_out)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.conv(input)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "\n",
    "        layer_config = ((64, 2), (64, 1), (128, 2), (128, 1))\n",
    "\n",
    "        ch_in = 1\n",
    "        block_list = []\n",
    "        for ch_out, stride in layer_config:\n",
    "            block = ConvBlock(ch_in, ch_out, stride)\n",
    "            block_list.append(block)\n",
    "            ch_in = ch_out\n",
    "\n",
    "        self.backbone = nn.Sequential(*block_list)\n",
    "\n",
    "        ### Add bottleneck layer  ###\n",
    "        ch_bn = 2  # number of channels in bottleneck\n",
    "        # it is called neck because it's between backbone and head\n",
    "        self.neck = nn.Linear(layer_config[-1][0], ch_bn)\n",
    "\n",
    "        self.head = nn.Linear(ch_bn, num_classes)\n",
    "\n",
    "    def forward(self, input):\n",
    "        featuremap = self.backbone(input)\n",
    "        squashed = F.adaptive_avg_pool2d(featuremap, output_size=(1, 1))\n",
    "        squeezed = squashed.view(squashed.shape[0], -1)\n",
    "\n",
    "        # Save bottleneck result in class member\n",
    "        self.neck_res = self.neck(squeezed)\n",
    "\n",
    "        pred = self.head(self.neck_res)\n",
    "        return pred\n",
    "\n",
    "    @classmethod\n",
    "    def loss(cls, pred, gt):\n",
    "        return F.cross_entropy(pred, gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.train_transform = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomResizedCrop(size=(28, 28), scale=(0.7, 1.1)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        self.val_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "        train_dataset = FashionMNIST(\"./data\", train=True,\n",
    "                                     transform=self.train_transform,\n",
    "                                     download=True)\n",
    "        val_dataset = FashionMNIST(\"./data\", train=False,\n",
    "                                   transform=self.val_transform,\n",
    "                                   download=True)\n",
    "\n",
    "        # Save val_dataset in class member\n",
    "        self.val_dataset = val_dataset\n",
    "\n",
    "        # Save 10 samples of each class\n",
    "        self.samples = defaultdict(list)\n",
    "        for idx, sample in enumerate(val_dataset):\n",
    "            _, label = sample\n",
    "            if len(self.samples[label]) < 10:\n",
    "                self.samples[label].append(val_dataset[idx])\n",
    "\n",
    "        self.plots_data = []\n",
    "\n",
    "        batch_size = 1024\n",
    "        self.train_loader = data.DataLoader(train_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True, num_workers=4)\n",
    "        self.val_loader = data.DataLoader(val_dataset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=4)\n",
    "\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        self.net = NeuralNet()\n",
    "        self.net.to(self.device)\n",
    "\n",
    "        self.logger = SummaryWriter()\n",
    "        self.i_batch = 0\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        num_epochs = ENUM\n",
    "\n",
    "        optimizer = torch.optim.Adam(self.net.parameters(), lr=1e-3)\n",
    "\n",
    "        for i_epoch in range(num_epochs):\n",
    "            self.i_epoch = i_epoch\n",
    "            self.net.train()\n",
    "\n",
    "            for feature_batch, gt_batch in self.train_loader:\n",
    "                feature_batch = feature_batch.to(self.device)\n",
    "                gt_batch = gt_batch.to(self.device)\n",
    "\n",
    "                pred_batch = self.net(feature_batch)\n",
    "\n",
    "                loss = NeuralNet.loss(pred_batch, gt_batch)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                self.logger.add_scalar(\"train/loss\", loss.item(), self.i_batch)\n",
    "\n",
    "                if self.i_batch % 100 == 0:\n",
    "                    print(f\"batch={self.i_batch} loss={loss.item():.6f}\")\n",
    "\n",
    "                self.i_batch += 1\n",
    "\n",
    "            self.validate()\n",
    "\n",
    "    def validate(self):\n",
    "        self.net.eval()\n",
    "\n",
    "        loss_all = []\n",
    "        pred_all = []\n",
    "        gt_all = []\n",
    "\n",
    "        self.save_plots_data()\n",
    "\n",
    "        for feature_batch, gt_batch in self.val_loader:\n",
    "            feature_batch = feature_batch.to(self.device)\n",
    "            gt_batch = gt_batch.to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                pred_batch = self.net(feature_batch)\n",
    "                loss = NeuralNet.loss(pred_batch, gt_batch)\n",
    "\n",
    "            loss_all.append(loss.item())\n",
    "            pred_all.append(pred_batch.cpu().numpy())\n",
    "            gt_all.append(gt_batch.cpu().numpy())\n",
    "\n",
    "        loss_mean = np.mean(np.array(loss_all))\n",
    "        pred_all = np.argmax(np.concatenate(pred_all, axis=0), axis=1)\n",
    "        gt_all = np.concatenate(np.array(gt_all))\n",
    "\n",
    "        accuracy = np.sum(np.equal(pred_all, gt_all)) / len(pred_all)\n",
    "\n",
    "        self.logger.add_scalar(\"val/loss\", loss_mean, self.i_batch)\n",
    "        self.logger.add_scalar(\"val/accuracy\", accuracy, self.i_batch)\n",
    "\n",
    "        print(f\"Val_loss={loss_mean} val_accu={accuracy:.6f}\")\n",
    "\n",
    "    def save_plots_data(self):\n",
    "        # Run through saved samples and save plot data\n",
    "        for label, samples in self.samples.items():\n",
    "            all_samples = [sample[0][None, :, :, :] for sample in samples]\n",
    "            feature_batch = torch.cat(all_samples, 0).to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                self.net(feature_batch).to(self.device)\n",
    "\n",
    "            xs = self.net.neck_res[:, 0].cpu().detach().numpy()\n",
    "            ys = self.net.neck_res[:, 1].cpu().detach().numpy()\n",
    "\n",
    "            for idx, (x, y) in enumerate(zip(xs, ys)):\n",
    "                self.plots_data.append((self.i_epoch, label, idx, x, y))\n",
    "                # print(self.plots_data[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch=0 loss=2.360684\n",
      "Val_loss=1.6073572397232057 val_accu=0.378800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_201692/1965652852.py:100: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  gt_all = np.concatenate(np.array(gt_all))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch=100 loss=1.429117\n",
      "Val_loss=1.3689235329627991 val_accu=0.471400\n",
      "Val_loss=1.1820245742797852 val_accu=0.565700\n",
      "batch=200 loss=1.154199\n",
      "Val_loss=1.0412736535072327 val_accu=0.625300\n",
      "Val_loss=1.0489192128181457 val_accu=0.612600\n",
      "batch=300 loss=0.893327\n",
      "Val_loss=0.8580572843551636 val_accu=0.711100\n",
      "batch=400 loss=0.869773\n",
      "Val_loss=0.8164847195148468 val_accu=0.773900\n",
      "Val_loss=0.8158915162086486 val_accu=0.772000\n",
      "batch=500 loss=0.782626\n",
      "Val_loss=0.7533595383167266 val_accu=0.788600\n",
      "Val_loss=0.753343665599823 val_accu=0.787700\n",
      "batch=600 loss=0.745082\n",
      "Val_loss=0.7192492365837098 val_accu=0.794200\n",
      "batch=700 loss=0.714647\n",
      "Val_loss=0.788089120388031 val_accu=0.771100\n",
      "Val_loss=0.8862158298492432 val_accu=0.738300\n",
      "batch=800 loss=0.728464\n",
      "Val_loss=0.8215635299682618 val_accu=0.768000\n",
      "Val_loss=0.6726749956607818 val_accu=0.809900\n",
      "batch=900 loss=0.667508\n",
      "Val_loss=0.7473507940769195 val_accu=0.795000\n",
      "batch=1000 loss=0.671206\n",
      "Val_loss=0.673649662733078 val_accu=0.810300\n",
      "Val_loss=0.623761647939682 val_accu=0.833700\n",
      "batch=1100 loss=0.651183\n",
      "Val_loss=0.6030399203300476 val_accu=0.840300\n",
      "Val_loss=0.5973087549209595 val_accu=0.836900\n",
      "batch=1200 loss=0.587664\n",
      "Val_loss=0.6443873643875122 val_accu=0.824100\n",
      "Val_loss=0.6188427448272705 val_accu=0.840700\n",
      "batch=1300 loss=0.589078\n",
      "Val_loss=0.6103311955928803 val_accu=0.837100\n",
      "batch=1400 loss=0.578611\n",
      "Val_loss=0.6160802245140076 val_accu=0.836500\n",
      "Val_loss=0.576997047662735 val_accu=0.842100\n",
      "batch=1500 loss=0.581708\n",
      "Val_loss=0.56612588763237 val_accu=0.845700\n",
      "Val_loss=0.5601002991199493 val_accu=0.845100\n",
      "batch=1600 loss=0.621667\n",
      "Val_loss=0.5683958739042282 val_accu=0.848600\n",
      "batch=1700 loss=0.544767\n",
      "Val_loss=0.5413100361824036 val_accu=0.851700\n",
      "Val_loss=0.5596337527036667 val_accu=0.850000\n",
      "batch=1800 loss=0.532146\n",
      "Val_loss=0.5621411085128785 val_accu=0.851700\n",
      "Val_loss=0.5494130283594132 val_accu=0.855700\n",
      "batch=1900 loss=0.558191\n",
      "Val_loss=0.5269671857357026 val_accu=0.861500\n",
      "batch=2000 loss=0.547811\n",
      "Val_loss=0.5912659049034119 val_accu=0.842100\n",
      "Val_loss=0.5290268301963806 val_accu=0.858400\n",
      "batch=2100 loss=0.561896\n",
      "Val_loss=0.5305476218461991 val_accu=0.859900\n",
      "Val_loss=0.528148177266121 val_accu=0.858600\n",
      "batch=2200 loss=0.485983\n",
      "Val_loss=0.532958808541298 val_accu=0.851700\n",
      "batch=2300 loss=0.549502\n",
      "Val_loss=0.5024442702531815 val_accu=0.865000\n",
      "Val_loss=0.5195604234933853 val_accu=0.859000\n",
      "batch=2400 loss=0.541666\n",
      "Val_loss=0.564031544327736 val_accu=0.853300\n",
      "Val_loss=0.5120665162801743 val_accu=0.865200\n",
      "batch=2500 loss=0.538114\n",
      "Val_loss=0.5218361407518387 val_accu=0.861500\n",
      "Val_loss=0.5155634671449661 val_accu=0.854400\n",
      "batch=2600 loss=0.425652\n",
      "Val_loss=0.49660069644451144 val_accu=0.867100\n",
      "batch=2700 loss=0.549928\n",
      "Val_loss=0.5610180258750915 val_accu=0.848000\n",
      "Val_loss=0.5279792517423629 val_accu=0.857700\n",
      "batch=2800 loss=0.518370\n",
      "Val_loss=0.484952574968338 val_accu=0.869800\n",
      "Val_loss=0.4728909432888031 val_accu=0.873700\n",
      "batch=2900 loss=0.480186\n",
      "Val_loss=0.5033192366361618 val_accu=0.860300\n",
      "batch=3000 loss=0.479200\n",
      "Val_loss=0.48900699615478516 val_accu=0.870600\n",
      "Val_loss=0.5278510808944702 val_accu=0.861900\n",
      "batch=3100 loss=0.461401\n",
      "Val_loss=0.5352049589157104 val_accu=0.860300\n",
      "Val_loss=0.49236143231391905 val_accu=0.873800\n",
      "batch=3200 loss=0.507285\n",
      "Val_loss=0.4691730797290802 val_accu=0.874500\n",
      "batch=3300 loss=0.530798\n",
      "Val_loss=0.4935419261455536 val_accu=0.871900\n",
      "Val_loss=0.5051461756229401 val_accu=0.871800\n",
      "batch=3400 loss=0.481493\n",
      "Val_loss=0.4908842951059341 val_accu=0.870100\n",
      "Val_loss=0.4934785783290863 val_accu=0.878200\n",
      "batch=3500 loss=0.411058\n",
      "Val_loss=0.4661386102437973 val_accu=0.873900\n",
      "Val_loss=0.4725020796060562 val_accu=0.874800\n",
      "batch=3600 loss=0.471008\n",
      "Val_loss=0.4926507413387299 val_accu=0.874400\n",
      "batch=3700 loss=0.423549\n",
      "Val_loss=0.46600702702999114 val_accu=0.877700\n",
      "Val_loss=0.44218155443668367 val_accu=0.882100\n",
      "batch=3800 loss=0.422402\n",
      "Val_loss=0.4643772214651108 val_accu=0.879400\n",
      "Val_loss=0.4823864817619324 val_accu=0.868400\n",
      "batch=3900 loss=0.409164\n",
      "Val_loss=0.47097915410995483 val_accu=0.875000\n",
      "batch=4000 loss=0.416130\n",
      "Val_loss=0.44443916976451875 val_accu=0.883600\n",
      "Val_loss=0.45328624844551085 val_accu=0.882600\n",
      "batch=4100 loss=0.489132\n",
      "Val_loss=0.47946651577949523 val_accu=0.879300\n",
      "Val_loss=0.4567966550588608 val_accu=0.882600\n",
      "batch=4200 loss=0.417129\n",
      "Val_loss=0.4794052571058273 val_accu=0.872500\n",
      "batch=4300 loss=0.449705\n",
      "Val_loss=0.45851232409477233 val_accu=0.880400\n",
      "Val_loss=0.4768277645111084 val_accu=0.875900\n",
      "batch=4400 loss=0.411671\n",
      "Val_loss=0.4543844312429428 val_accu=0.884700\n",
      "Val_loss=0.44771736264228823 val_accu=0.880900\n",
      "batch=4500 loss=0.413468\n",
      "Val_loss=0.46047156155109403 val_accu=0.878700\n",
      "batch=4600 loss=0.418151\n",
      "Val_loss=0.4459266871213913 val_accu=0.881400\n",
      "Val_loss=0.44534837901592256 val_accu=0.885300\n",
      "batch=4700 loss=0.389837\n",
      "Val_loss=0.47415131628513335 val_accu=0.880900\n",
      "Val_loss=0.46301066875457764 val_accu=0.881100\n",
      "batch=4800 loss=0.391053\n",
      "Val_loss=0.44058857262134554 val_accu=0.883800\n",
      "Val_loss=0.48414810597896574 val_accu=0.876000\n",
      "batch=4900 loss=0.433204\n",
      "Val_loss=0.46916404366493225 val_accu=0.872900\n",
      "batch=5000 loss=0.411080\n",
      "Val_loss=0.4569999545812607 val_accu=0.882200\n",
      "Val_loss=0.438091441988945 val_accu=0.889800\n",
      "batch=5100 loss=0.401395\n",
      "Val_loss=0.4768407613039017 val_accu=0.881900\n",
      "Val_loss=0.458413565158844 val_accu=0.876900\n",
      "batch=5200 loss=0.357067\n",
      "Val_loss=0.4705876260995865 val_accu=0.884000\n",
      "batch=5300 loss=0.372001\n",
      "Val_loss=0.4552462786436081 val_accu=0.885200\n",
      "Val_loss=0.4506951034069061 val_accu=0.886100\n",
      "batch=5400 loss=0.366364\n",
      "Val_loss=0.4450407654047012 val_accu=0.882800\n",
      "Val_loss=0.44678099155426027 val_accu=0.880200\n",
      "batch=5500 loss=0.389067\n",
      "Val_loss=0.46200085878372193 val_accu=0.879200\n",
      "batch=5600 loss=0.444097\n",
      "Val_loss=0.4372837543487549 val_accu=0.882200\n",
      "Val_loss=0.43578543066978453 val_accu=0.888000\n",
      "batch=5700 loss=0.391536\n",
      "Val_loss=0.45551695823669436 val_accu=0.885800\n",
      "Val_loss=0.43337402641773226 val_accu=0.888700\n",
      "batch=5800 loss=0.381542\n",
      "Val_loss=0.4783194363117218 val_accu=0.875000\n",
      "Val_loss=0.43497025072574613 val_accu=0.888900\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer = Trainer()\n",
    "trainer.train()\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "import pathlib as pl\n",
    "from datetime import datetime\n",
    "\n",
    "plt.style.use('Solarize_Light2')\n",
    "\n",
    "plots_data = trainer.plots_data\n",
    "\n",
    "# save plots\n",
    "plots_path = pl.Path(\"./plots/\")\n",
    "plots_path = plots_path / datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "plots_path.mkdir()\n",
    "\n",
    "for i in range(ENUM):\n",
    "    cur_path = plots_path / f\"e{i:02}.png\"\n",
    "\n",
    "    # get all plots' data for current epoch\n",
    "    cur_epoch_data = [plot_data for j, plot_data in enumerate(\n",
    "        plots_data) if plot_data[0] == i]\n",
    "\n",
    "    figure(figsize=(12, 10), dpi=80)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.xlim(-60, 60)\n",
    "    plt.ylim(-130, 90)\n",
    "\n",
    "    for label, (color, name) in enumerate(zip(COLORS, NAMES)):\n",
    "        x, y = [], []\n",
    "\n",
    "        for plot_data in cur_epoch_data:\n",
    "            if plot_data[1] != label:\n",
    "                continue\n",
    "\n",
    "            for idx in range(10):\n",
    "                if plot_data[2] != idx:\n",
    "                    continue\n",
    "\n",
    "                x.append(plot_data[3])\n",
    "                y.append(plot_data[4])\n",
    "\n",
    "        plt.scatter(x, y, c=color, label=name)\n",
    "\n",
    "    plt.title(f\"epoch {i:02}\", fontsize=24)\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.savefig(cur_path)\n",
    "    plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
